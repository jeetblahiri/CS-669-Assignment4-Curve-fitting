# -*- coding: utf-8 -*-
"""Linear model for curve_Q2-final-Assignment4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m3l2prThquhxRxJyAKNkC6ljSBXyLRRT
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from mpl_toolkits.mplot3d import Axes3D

# Load your CSV file (replace 'your_file.csv' with your actual file)
df = pd.read_csv('/content/drive/MyDrive/Regression/BivariateData/13.csv', header=None, names=['x1', 'x2', 'y'])

# Split the data into training and test sets (70:30 ratio)
train_data, test_data = train_test_split(df, test_size=0.3, random_state=42)

# Extract features and target variable
X_train = train_data[['x1', 'x2']]
y_train = train_data['y']
X_test = test_data[['x1', 'x2']]
y_test = test_data['y']

# Function to create Gaussian basis functions
def gaussian_basis_functions(data, centers, sigma):
    return np.exp(-np.linalg.norm(data - centers, axis=1)**2 / (2 * sigma**2))

# Number of basis functions for different model complexities
model_complexities = [2, 4, 8, 16, 32, 128, 256]

# Plot arrays to store MSE values for training and test data
mse_train_values = []
mse_test_values = []

# Loop through different model complexities
for complexity in model_complexities:
    # Apply K-means clustering to find centers
    kmeans = KMeans(n_clusters=complexity, random_state=42)
    kmeans.fit(X_train)
    centers = kmeans.cluster_centers_

    # Create design matrix using Gaussian basis functions
    phi_train = np.array([gaussian_basis_functions(x, centers, sigma=1.0) for x in X_train.values])
    phi_test = np.array([gaussian_basis_functions(x, centers, sigma=1.0) for x in X_test.values])

    # Train linear regression model
    model = LinearRegression()
    model.fit(phi_train, y_train)

    # Make predictions on training and test data
    y_train_pred = model.predict(phi_train)
    y_test_pred = model.predict(phi_test)

    # Calculate MSE for training and test data
    mse_train = mean_squared_error(y_train, y_train_pred)
    mse_test = mean_squared_error(y_test, y_test_pred)

    print("Train MSE: ", mse_train)
    print("Test MSE:", mse_test)

    # Append MSE values to the plot arrays
    mse_train_values.append(mse_train)
    mse_test_values.append(mse_test)

    # Plot 3D model output and target output for training data
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(X_train['x1'], X_train['x2'], y_train, c='r', marker='o', label='Target Output')
    ax.scatter(X_train['x1'], X_train['x2'], y_train_pred, c='b', marker='^', label='Model Output')
    ax.set_xlabel('x1')
    ax.set_ylabel('x2')
    ax.set_zlabel('y')
    ax.legend()
    plt.title(f'Training Data - Model Complexity: {complexity}')
    plt.show()

    # Plot 3D model output and target output for test data
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(X_test['x1'], X_test['x2'], y_test, c='r', marker='o', label='Target Output')
    ax.scatter(X_test['x1'], X_test['x2'], y_test_pred, c='b', marker='^', label='Model Output')
    ax.set_xlabel('x1')
    ax.set_ylabel('x2')
    ax.set_zlabel('y')
    ax.legend()
    plt.title(f'Test Data - Model Complexity: {complexity}')
    plt.show()

# Plot MSE values for different model complexities
plt.plot(model_complexities, mse_train_values, marker='o', label='Training Data')
plt.plot(model_complexities, mse_test_values, marker='o', label='Test Data')
plt.xscale('log')
plt.xlabel('Model Complexity (Number of Basis Functions)')
plt.ylabel('Mean Squared Error (MSE)')
plt.legend()
plt.title('MSE on Training and Test Data for Different Model Complexities')
plt.show()

"""# Regularized

### alpha = 0.001
"""

from sklearn.linear_model import Ridge

# Regularization parameter (adjust as needed)
alpha = 0.001

# Arrays to store MSE values for training and test data with regularization
mse_train_values_reg = []
mse_test_values_reg = []

# Loop through different model complexities
for complexity in model_complexities:
    # Apply K-means clustering to find centers
    kmeans = KMeans(n_clusters=complexity, random_state=42)
    kmeans.fit(X_train)
    centers = kmeans.cluster_centers_

    # Create design matrix using Gaussian basis functions
    phi_train = np.array([gaussian_basis_functions(x, centers, sigma=1.0) for x in X_train.values])
    phi_test = np.array([gaussian_basis_functions(x, centers, sigma=1.0) for x in X_test.values])

    # Train Ridge regression model
    model = Ridge(alpha=alpha)
    model.fit(phi_train, y_train)

    # Make predictions on training and test data
    y_train_pred = model.predict(phi_train)
    y_test_pred = model.predict(phi_test)

    # Calculate MSE for training and test data
    mse_train = mean_squared_error(y_train, y_train_pred)
    mse_test = mean_squared_error(y_test, y_test_pred)

    # Append MSE values to the plot arrays
    mse_train_values_reg.append(mse_train)
    mse_test_values_reg.append(mse_test)


    print("Train MSE: ", mse_train)
    print("Test MSE:", mse_test)

    # Plot 3D model output and target output for training data
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(X_train['x1'], X_train['x2'], y_train, c='r', marker='o', label='Target Output')
    ax.scatter(X_train['x1'], X_train['x2'], y_train_pred, c='b', marker='^', label='Model Output')
    ax.set_xlabel('x1')
    ax.set_ylabel('x2')
    ax.set_zlabel('y')
    ax.legend()
    plt.title(f'Training Data - Model Complexity: {complexity} (Regularized)')
    plt.show()

    # Plot 3D model output and target output for test data
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(X_test['x1'], X_test['x2'], y_test, c='r', marker='o', label='Target Output')
    ax.scatter(X_test['x1'], X_test['x2'], y_test_pred, c='b', marker='^', label='Model Output')
    ax.set_xlabel('x1')
    ax.set_ylabel('x2')
    ax.set_zlabel('y')
    ax.legend()
    plt.title(f'Test Data - Model Complexity: {complexity} (Regularized)')
    plt.show()

# Plot MSE values for different model complexities with regularization
plt.plot(model_complexities, mse_train_values_reg, marker='o', label='Training Data (Regularized)')
plt.plot(model_complexities, mse_test_values_reg, marker='o', label='Test Data (Regularized)')
plt.xscale('log')
plt.xlabel('Model Complexity (Number of Basis Functions)')
plt.ylabel('Mean Squared Error (MSE)')
plt.legend()
plt.title('MSE on Training and Test Data for Different Model Complexities with Regularization')
plt.show()

"""

### alpha=0.01"""

from sklearn.linear_model import Ridge

# Regularization parameter (adjust as needed)
alpha = 0.01

# Arrays to store MSE values for training and test data with regularization
mse_train_values_reg = []
mse_test_values_reg = []

# Loop through different model complexities
for complexity in model_complexities:
    # Apply K-means clustering to find centers
    kmeans = KMeans(n_clusters=complexity, random_state=42)
    kmeans.fit(X_train)
    centers = kmeans.cluster_centers_

    # Create design matrix using Gaussian basis functions
    phi_train = np.array([gaussian_basis_functions(x, centers, sigma=1.0) for x in X_train.values])
    phi_test = np.array([gaussian_basis_functions(x, centers, sigma=1.0) for x in X_test.values])

    # Train Ridge regression model
    model = Ridge(alpha=alpha)
    model.fit(phi_train, y_train)

    # Make predictions on training and test data
    y_train_pred = model.predict(phi_train)
    y_test_pred = model.predict(phi_test)

    # Calculate MSE for training and test data
    mse_train = mean_squared_error(y_train, y_train_pred)
    mse_test = mean_squared_error(y_test, y_test_pred)

    # Append MSE values to the plot arrays
    mse_train_values_reg.append(mse_train)
    mse_test_values_reg.append(mse_test)


    print("Train MSE: ", mse_train)
    print("Test MSE:", mse_test)

    # Plot 3D model output and target output for training data
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(X_train['x1'], X_train['x2'], y_train, c='r', marker='o', label='Target Output')
    ax.scatter(X_train['x1'], X_train['x2'], y_train_pred, c='b', marker='^', label='Model Output')
    ax.set_xlabel('x1')
    ax.set_ylabel('x2')
    ax.set_zlabel('y')
    ax.legend()
    plt.title(f'Training Data - Model Complexity: {complexity} (Regularized)')
    plt.show()

    # Plot 3D model output and target output for test data
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(X_test['x1'], X_test['x2'], y_test, c='r', marker='o', label='Target Output')
    ax.scatter(X_test['x1'], X_test['x2'], y_test_pred, c='b', marker='^', label='Model Output')
    ax.set_xlabel('x1')
    ax.set_ylabel('x2')
    ax.set_zlabel('y')
    ax.legend()
    plt.title(f'Test Data - Model Complexity: {complexity} (Regularized)')
    plt.show()

# Plot MSE values for different model complexities with regularization
plt.plot(model_complexities, mse_train_values_reg, marker='o', label='Training Data (Regularized)')
plt.plot(model_complexities, mse_test_values_reg, marker='o', label='Test Data (Regularized)')
plt.xscale('log')
plt.xlabel('Model Complexity (Number of Basis Functions)')
plt.ylabel('Mean Squared Error (MSE)')
plt.legend()
plt.title('MSE on Training and Test Data for Different Model Complexities with Regularization')
plt.show()

"""

### alpha=0.05"""

from sklearn.linear_model import Ridge

# Regularization parameter (adjust as needed)
alpha = 0.05

# Arrays to store MSE values for training and test data with regularization
mse_train_values_reg = []
mse_test_values_reg = []

# Loop through different model complexities
for complexity in model_complexities:
    # Apply K-means clustering to find centers
    kmeans = KMeans(n_clusters=complexity, random_state=42)
    kmeans.fit(X_train)
    centers = kmeans.cluster_centers_

    # Create design matrix using Gaussian basis functions
    phi_train = np.array([gaussian_basis_functions(x, centers, sigma=1.0) for x in X_train.values])
    phi_test = np.array([gaussian_basis_functions(x, centers, sigma=1.0) for x in X_test.values])

    # Train Ridge regression model
    model = Ridge(alpha=alpha)
    model.fit(phi_train, y_train)

    # Make predictions on training and test data
    y_train_pred = model.predict(phi_train)
    y_test_pred = model.predict(phi_test)

    # Calculate MSE for training and test data
    mse_train = mean_squared_error(y_train, y_train_pred)
    mse_test = mean_squared_error(y_test, y_test_pred)

    # Append MSE values to the plot arrays
    mse_train_values_reg.append(mse_train)
    mse_test_values_reg.append(mse_test)


    print("Train MSE: ", mse_train)
    print("Test MSE:", mse_test)

    # Plot 3D model output and target output for training data
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(X_train['x1'], X_train['x2'], y_train, c='r', marker='o', label='Target Output')
    ax.scatter(X_train['x1'], X_train['x2'], y_train_pred, c='b', marker='^', label='Model Output')
    ax.set_xlabel('x1')
    ax.set_ylabel('x2')
    ax.set_zlabel('y')
    ax.legend()
    plt.title(f'Training Data - Model Complexity: {complexity} (Regularized)')
    plt.show()

    # Plot 3D model output and target output for test data
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(X_test['x1'], X_test['x2'], y_test, c='r', marker='o', label='Target Output')
    ax.scatter(X_test['x1'], X_test['x2'], y_test_pred, c='b', marker='^', label='Model Output')
    ax.set_xlabel('x1')
    ax.set_ylabel('x2')
    ax.set_zlabel('y')
    ax.legend()
    plt.title(f'Test Data - Model Complexity: {complexity} (Regularized)')
    plt.show()

# Plot MSE values for different model complexities with regularization
plt.plot(model_complexities, mse_train_values_reg, marker='o', label='Training Data (Regularized)')
plt.plot(model_complexities, mse_test_values_reg, marker='o', label='Test Data (Regularized)')
plt.xscale('log')
plt.xlabel('Model Complexity (Number of Basis Functions)')
plt.ylabel('Mean Squared Error (MSE)')
plt.legend()
plt.title('MSE on Training and Test Data for Different Model Complexities with Regularization')
plt.show()

"""### alpha = 0.1"""

from sklearn.linear_model import Ridge

# Regularization parameter (adjust as needed)
alpha = 0.1

# Arrays to store MSE values for training and test data with regularization
mse_train_values_reg = []
mse_test_values_reg = []

# Loop through different model complexities
for complexity in model_complexities:
    # Apply K-means clustering to find centers
    kmeans = KMeans(n_clusters=complexity, random_state=42)
    kmeans.fit(X_train)
    centers = kmeans.cluster_centers_

    # Create design matrix using Gaussian basis functions
    phi_train = np.array([gaussian_basis_functions(x, centers, sigma=1.0) for x in X_train.values])
    phi_test = np.array([gaussian_basis_functions(x, centers, sigma=1.0) for x in X_test.values])

    # Train Ridge regression model
    model = Ridge(alpha=alpha)
    model.fit(phi_train, y_train)

    # Make predictions on training and test data
    y_train_pred = model.predict(phi_train)
    y_test_pred = model.predict(phi_test)

    # Calculate MSE for training and test data
    mse_train = mean_squared_error(y_train, y_train_pred)
    mse_test = mean_squared_error(y_test, y_test_pred)

    # Append MSE values to the plot arrays
    mse_train_values_reg.append(mse_train)
    mse_test_values_reg.append(mse_test)


    print("Train MSE: ", mse_train)
    print("Test MSE:", mse_test)

    # Plot 3D model output and target output for training data
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(X_train['x1'], X_train['x2'], y_train, c='r', marker='o', label='Target Output')
    ax.scatter(X_train['x1'], X_train['x2'], y_train_pred, c='b', marker='^', label='Model Output')
    ax.set_xlabel('x1')
    ax.set_ylabel('x2')
    ax.set_zlabel('y')
    ax.legend()
    plt.title(f'Training Data - Model Complexity: {complexity} (Regularized)')
    plt.show()

    # Plot 3D model output and target output for test data
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(X_test['x1'], X_test['x2'], y_test, c='r', marker='o', label='Target Output')
    ax.scatter(X_test['x1'], X_test['x2'], y_test_pred, c='b', marker='^', label='Model Output')
    ax.set_xlabel('x1')
    ax.set_ylabel('x2')
    ax.set_zlabel('y')
    ax.legend()
    plt.title(f'Test Data - Model Complexity: {complexity} (Regularized)')
    plt.show()

# Plot MSE values for different model complexities with regularization
plt.plot(model_complexities, mse_train_values_reg, marker='o', label='Training Data (Regularized)')
plt.plot(model_complexities, mse_test_values_reg, marker='o', label='Test Data (Regularized)')
plt.xscale('log')
plt.xlabel('Model Complexity (Number of Basis Functions)')
plt.ylabel('Mean Squared Error (MSE)')
plt.legend()
plt.title('MSE on Training and Test Data for Different Model Complexities with Regularization')
plt.show()

"""### alpha = 1"""



from sklearn.linear_model import Ridge

# Regularization parameter (adjust as needed)
alpha = 1

# Arrays to store MSE values for training and test data with regularization
mse_train_values_reg = []
mse_test_values_reg = []

# Loop through different model complexities
for complexity in model_complexities:
    # Apply K-means clustering to find centers
    kmeans = KMeans(n_clusters=complexity, random_state=42)
    kmeans.fit(X_train)
    centers = kmeans.cluster_centers_

    # Create design matrix using Gaussian basis functions
    phi_train = np.array([gaussian_basis_functions(x, centers, sigma=1.0) for x in X_train.values])
    phi_test = np.array([gaussian_basis_functions(x, centers, sigma=1.0) for x in X_test.values])

    # Train Ridge regression model
    model = Ridge(alpha=alpha)
    model.fit(phi_train, y_train)

    # Make predictions on training and test data
    y_train_pred = model.predict(phi_train)
    y_test_pred = model.predict(phi_test)

    # Calculate MSE for training and test data
    mse_train = mean_squared_error(y_train, y_train_pred)
    mse_test = mean_squared_error(y_test, y_test_pred)

    # Append MSE values to the plot arrays
    mse_train_values_reg.append(mse_train)
    mse_test_values_reg.append(mse_test)


    print("Train MSE: ", mse_train)
    print("Test MSE:", mse_test)

    # Plot 3D model output and target output for training data
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(X_train['x1'], X_train['x2'], y_train, c='r', marker='o', label='Target Output')
    ax.scatter(X_train['x1'], X_train['x2'], y_train_pred, c='b', marker='^', label='Model Output')
    ax.set_xlabel('x1')
    ax.set_ylabel('x2')
    ax.set_zlabel('y')
    ax.legend()
    plt.title(f'Training Data - Model Complexity: {complexity} (Regularized)')
    plt.show()

    # Plot 3D model output and target output for test data
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(X_test['x1'], X_test['x2'], y_test, c='r', marker='o', label='Target Output')
    ax.scatter(X_test['x1'], X_test['x2'], y_test_pred, c='b', marker='^', label='Model Output')
    ax.set_xlabel('x1')
    ax.set_ylabel('x2')
    ax.set_zlabel('y')
    ax.legend()
    plt.title(f'Test Data - Model Complexity: {complexity} (Regularized)')
    plt.show()

# Plot MSE values for different model complexities with regularization
plt.plot(model_complexities, mse_train_values_reg, marker='o', label='Training Data (Regularized)')
plt.plot(model_complexities, mse_test_values_reg, marker='o', label='Test Data (Regularized)')
plt.xscale('log')
plt.xlabel('Model Complexity (Number of Basis Functions)')
plt.ylabel('Mean Squared Error (MSE)')
plt.legend()
plt.title('MSE on Training and Test Data for Different Model Complexities with Regularization')
plt.show()

